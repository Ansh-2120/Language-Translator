# English ‚Üí French Language Translator üåç

A neural machine translation system that translates English sentences to French using a Seq2Seq model with Attention mechanism, trained on 200,000+ parallel sentence pairs from Europarl and News Commentary datasets.

## üìå What It Does

This translator converts English sentences into French using deep learning. It handles:
- **Simple sentences**: "Hello, how are you?" ‚Üí "bonjour, comment allez-vous?"
- **Complex phrases**: Long sentences with proper grammar
- **Real-time translation**: Interactive command-line interface

## üõ†Ô∏è Tech Stack

**Frameworks & Libraries:**
- **TensorFlow/Keras** - Neural network framework
- **Spacy** - Tokenization (en_core_web_sm, fr_core_news_sm)
- **Pandas** - Data processing
- **NumPy** - Numerical operations
- **Scikit-learn** - Data shuffling

**Model Architecture:**
- **Type**: Sequence-to-Sequence with Attention
- **Encoder**: LSTM (256 units) with embeddings (128 dim)
- **Decoder**: LSTM (256 units) with attention mechanism
- **Attention**: Dot-product attention for context vectors
- **Vocab Size**: ~30K English, ~45K French tokens

## ‚öôÔ∏è How It Works

### **Tech Flow**

```
Raw Datasets (Europarl + News Commentary)
    ‚Üì
preprocess.py ‚Üí Text cleaning & combining
    ‚Üì
nlp_process.py ‚Üí Tokenization with Spacy
    ‚Üì
nlp_vocab.py ‚Üí Build vocabularies & pad sequences
    ‚Üì
model.py ‚Üí Train Seq2Seq + Attention model
    ‚Üì
pipeline.py ‚Üí Interactive translation interface
```

### **1. Data Preprocessing** (`preprocess.py`)

**What happens:**
- Loads Europarl (2M+ pairs) and News Commentary (400K+ pairs) datasets
- Cleans text: lowercase, remove HTML tags, normalize Unicode
- Filters to 200K sentence pairs for training
- Shuffles and saves as CSV

**Key operations:**
```python
Text normalization ‚Üí HTML removal ‚Üí Lowercasing ‚Üí Whitespace cleanup
```

### **2. Tokenization** (`nlp_process.py`)

**What happens:**
- Uses Spacy for English and French tokenization
- Filters sentences (1-60 tokens, length ratio < 2.5)
- Adds special tokens: `<SOS>` (start) and `<EOS>` (end)
- Saves tokenized data as pickle

**Token flow:**
```
English: "I love coding"
    ‚Üì
Tokens: ['I', 'love', 'coding']

French: "J'aime coder"
    ‚Üì
Input:  ['<SOS>', "J'", 'aime', 'coder']
Output: ["J'", 'aime', 'coder', '<EOS>']
```

### **3. Vocabulary Building** (`nlp_vocab.py`)

**What happens:**
- Creates word-to-index mappings using Keras Tokenizer
- Pads sequences to max length (76 for EN, 84 for FR)
- Saves tokenizers and padded sequences

**Output:**
- `X_en`: English sequences (input)
- `Y_fr_in`: French sequences with `<SOS>` (decoder input)
- `Y_fr_out`: French sequences with `<EOS>` (target)

### **4. Model Training** (`model.py`)

**Architecture:**
```
ENCODER:
Input (max_len_en) ‚Üí Embedding(128) ‚Üí LSTM(256) ‚Üí [encoder_outputs, state_h, state_c]

DECODER:
Input (max_len_fr) ‚Üí Embedding(128) ‚Üí LSTM(256, initial_state=[h, c]) ‚Üí decoder_outputs
                                           ‚Üì
                              ATTENTION MECHANISM
                        (Dot product with encoder_outputs)
                                           ‚Üì
                              Context Vector + decoder_outputs
                                           ‚Üì
                              Dense(vocab_size_fr, softmax) ‚Üí French word probabilities
```

**Training config:**
- Optimizer: Adam
- Loss: Sparse categorical crossentropy
- Batch size: 32
- Epochs: 15
- Validation split: 10%

**Attention mechanism:**
- Calculates similarity between decoder and encoder outputs
- Creates context vector focusing on relevant source words
- Improves translation quality for long sentences

### **5. Translation Pipeline** (`pipeline.py`)

**Inference process:**
1. Tokenize English input
2. Encode with trained encoder
3. Decode word-by-word:
   - Start with `<SOS>` token
   - Predict next word using decoder + attention
   - Stop at `<EOS>` or max length
4. Convert token IDs back to French words

**Usage:**
```bash
python pipeline.py
```

## üìä Dataset

**Sources:**
- **Europarl v10**: European Parliament proceedings (~2M pairs)
- **News Commentary v18**: News articles (~400K pairs)

**Processing:**
- Combined: 2.4M+ sentence pairs
- Filtered to: 200K pairs (for faster training)
- Max sentence length: 60 tokens
- Length ratio threshold: 2.5x

## üöÄ Training Process

1. **Load & Clean Data** - Normalize, filter, shuffle
2. **Tokenize** - Spacy tokenization for EN/FR
3. **Build Vocabularies** - Word-to-index mappings
4. **Pad Sequences** - Fixed-length inputs for neural network
5. **Train Seq2Seq** - 15 epochs with attention mechanism
6. **Save Model** - Export as `nmt_model.h5`
7. **Interactive Translation** - Real-time CLI interface

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ preprocess.py              # Data loading and cleaning
‚îú‚îÄ‚îÄ nlp_process.py             # Spacy tokenization
‚îú‚îÄ‚îÄ nlp_vocab.py               # Vocabulary building & padding
‚îú‚îÄ‚îÄ model.py                   # Seq2Seq + Attention training
‚îú‚îÄ‚îÄ pipeline.py                # Translation inference
‚îú‚îÄ‚îÄ cleaned_en_fr.csv          # Preprocessed dataset
‚îú‚îÄ‚îÄ processed_en_fr_tokens.pkl # Tokenized data
‚îú‚îÄ‚îÄ tokenizer_en.pkl           # English vocabulary
‚îú‚îÄ‚îÄ tokenizer_fr.pkl           # French vocabulary
‚îú‚îÄ‚îÄ padded_sequences.pkl       # Padded input sequences
‚îî‚îÄ‚îÄ nmt_model.h5               # Trained model
```

## üéØ Model Details

**Parameters:**
- Embedding dimension: 128
- LSTM units: 256
- Encoder vocab: ~30,000 words
- Decoder vocab: ~45,000 words
- Max input length: 76 tokens
- Max output length: 84 tokens

**Architecture highlights:**
- Bidirectional information flow via attention
- Teacher forcing during training
- Autoregressive decoding during inference

## üíª Requirements

```
tensorflow>=2.x
spacy>=3.0
pandas
numpy
scikit-learn
```

**Spacy models:**
```bash
python -m spacy download en_core_web_sm
python -m spacy download fr_core_news_sm
```

## üîÆ Future Improvements

- Add Transformer architecture (better than LSTM)
- Increase dataset size for better accuracy
- Implement beam search for better translations
- Add BLEU score evaluation
- Create web interface with Flask/FastAPI
- Support more language pairs

---

**Built with Seq2Seq + Attention to bridge language barriers! üó£Ô∏è‚ú®**
